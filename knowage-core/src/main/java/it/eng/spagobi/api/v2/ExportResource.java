/*
 * Knowage, Open Source Business Intelligence suite
 * Copyright (C) 2016 Engineering Ingegneria Informatica S.p.A.

 * Knowage is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.

 * Knowage is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.

 * You should have received a copy of the GNU Affero General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package it.eng.spagobi.api.v2;

import java.io.IOException;
import java.nio.file.DirectoryStream;
import java.nio.file.Files;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.UUID;

import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import javax.ws.rs.Consumes;
import javax.ws.rs.DefaultValue;
import javax.ws.rs.GET;
import javax.ws.rs.POST;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.QueryParam;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.Response.Status;

import org.apache.log4j.Logger;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;
import org.json.JSONObjectDeserializator;
import org.quartz.JobDataMap;
import org.quartz.JobDetail;
import org.quartz.Scheduler;
import org.quartz.SchedulerException;
import org.quartz.impl.StdSchedulerFactory;

import it.eng.knowage.document.export.cockpit.CSVCockpitDataExporter;
import it.eng.spagobi.api.v2.export.Entry;
import it.eng.spagobi.api.v2.export.ExportDeleteOldJob;
import it.eng.spagobi.api.v2.export.ExportJobBuilder;
import it.eng.spagobi.api.v2.export.ExportMetadata;
import it.eng.spagobi.api.v2.export.ExportPathBuilder;
import it.eng.spagobi.api.v2.export.cockpit.DocumentExportConf;
import it.eng.spagobi.commons.bo.UserProfile;
import it.eng.spagobi.commons.utilities.SpagoBIUtilities;
import it.eng.spagobi.tools.dataset.constants.DataSetConstants;
import it.eng.spagobi.tools.dataset.service.ManageDataSetsForREST;
import it.eng.spagobi.user.UserProfileManager;

/**
 * Manage entity exported to file.
 *
 * @author Marco Libanori
 */
@Path("/2.0/export")
public class ExportResource {

	private static final String BODY_ATTR_PARAMETERS = "parameters";

	private static final String BODY_ATTR_DRIVERS = "drivers";

	private static final Logger logger = Logger.getLogger(ExportResource.class);

	@Context
	protected HttpServletRequest request;

	@Context
	protected HttpServletResponse response;

	/**
	 * List all exported files of a specific user.
	 *
	 * An {@link Entry} is generated by any directory that respect following condtions:
	 * <ul>
	 * <li>Contains a file with name metadata</li>
	 * <li>Contains a file with name data</li>
	 * <li>Contains an optional file with name downloaded</li>
	 * </ul>
	 *
	 * @return List of {@link Entry} with files exported by logged user
	 * @throws IOException
	 *             In case of errors during access of the filesystem
	 */
	@GET
	@Path("/dataset")
	@Produces(MediaType.APPLICATION_JSON)
	public List<Entry> dataset(@DefaultValue("false") @QueryParam("showAll") boolean showAll) throws IOException {

		logger.debug("IN");

		UserProfile userProfile = UserProfileManager.getProfile();
		String resoursePath = SpagoBIUtilities.getResourcePath();
		java.nio.file.Path perUserExportResourcePath = ExportPathBuilder.getInstance().getPerUserExportResourcePath(resoursePath, userProfile);

		List<Entry> ret = new ArrayList<Entry>();
		if (Files.isDirectory(perUserExportResourcePath)) {

			DirectoryStream<java.nio.file.Path> userJobDirectory = null;

			try {
				userJobDirectory = Files.newDirectoryStream(perUserExportResourcePath, new DirectoryStream.Filter<java.nio.file.Path>() {

					@Override
					public boolean accept(java.nio.file.Path entry) throws IOException {
						return Files.isDirectory(entry);
					}
				});

				Iterator<java.nio.file.Path> iterator = userJobDirectory.iterator();

				while (iterator.hasNext()) {
					java.nio.file.Path curr = iterator.next();
					java.nio.file.Path downloadPlaceholderPath = curr.resolve(ExportPathBuilder.DOWNLOADED_PLACEHOLDER_FILENAME);
					java.nio.file.Path metadataPath = curr.resolve(ExportPathBuilder.METADATA_FILENAME);
					java.nio.file.Path dataPath = curr.resolve(ExportPathBuilder.DATA_FILENAME);

					boolean downloadPlaceholderExist = Files.isRegularFile(downloadPlaceholderPath);

					if (!showAll) {
						if (downloadPlaceholderExist) {
							continue;
						}
					}

					if (!Files.isRegularFile(metadataPath)) {
						continue;
					}

					if (!Files.isRegularFile(dataPath)) {
						continue;
					}

					ExportMetadata metadata = ExportMetadata.readFromJsonFile(metadataPath);

					Entry entry = new Entry(metadata.getDataSetName(), metadata.getStartDate(), metadata.getId().toString(), downloadPlaceholderExist);

					ret.add(entry);
				}

			} finally {
				if (userJobDirectory != null) {
					try {
						userJobDirectory.close();
					} catch (IOException e) {
						// Yes, it's mute!
					}
				}
			}
		}

		logger.debug("OUT");

		return ret;
	}

	/**
	 * Schedules an export in CSV format of the dataset in input.
	 *
	 * @param dataSetId
	 *            Id of the dataset to be exported
	 * @param body
	 *            JSON that contains drivers and parameters data
	 * @return The job id
	 */
	@POST
	@Path("/dataset/{dataSetId}/csv")
	@Consumes(MediaType.APPLICATION_JSON)
	@Produces(MediaType.TEXT_PLAIN)
	public Response datasetAsCsv(@PathParam("dataSetId") Integer dataSetId, String body) {

		logger.debug("IN");

		JSONArray driversJson = null;
		JSONArray paramsJson = null;

		try {
			JSONObject data = new JSONObject(body);
			driversJson = data.has(BODY_ATTR_DRIVERS) ? data.getJSONArray(BODY_ATTR_DRIVERS) : null;
			paramsJson = data.has(BODY_ATTR_PARAMETERS) ? data.getJSONArray(BODY_ATTR_PARAMETERS) : null;
		} catch (JSONException e) {
			String msg = String.format("Body data is invalid: %s", body);
			logger.error(msg, e);
			return Response.serverError().build();
		}

		Response ret = null;
		Locale locale = request.getLocale();
		UserProfile userProfile = UserProfileManager.getProfile();
		Map<String, String> params = manageDataSetParameters(paramsJson);
		Map<String, Object> drivers = manageDataSetDrivers(driversJson);

		try {
			Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler();

			JobDetail exportJob = ExportJobBuilder.fromDataSetIdAndUserProfile(dataSetId, userProfile).withTypeOfCsv().withDrivers(drivers)
					.withParameters(params).withLocale(locale).build();

			scheduler.addJob(exportJob, true);
			scheduler.triggerJob(exportJob.getName(), exportJob.getGroup());

			ret = Response.ok().entity(exportJob.getName()).build();

			scheduleCleanUp();

		} catch (SchedulerException e) {
			String msg = String.format("Error during scheduling of export job for dataset %d", dataSetId);
			logger.error(msg, e);
			ret = Response.serverError().build();
		}

		logger.debug("OUT");

		return ret;
	}

	/**
	 * Schedules an export in Excel format of the dataset in input.
	 *
	 * @param dataSetId
	 *            Id of the dataset to be exported
	 * @param body
	 *            JSON that contains drivers and parameters data
	 * @return The job id
	 */
	@POST
	@Path("/dataset/{dataSetId}/xls")
	@Consumes(MediaType.APPLICATION_JSON)
	@Produces(MediaType.TEXT_PLAIN)
	public Response datasetAsXls(@PathParam("dataSetId") Integer dataSetId, String body) {

		logger.debug("IN");

		JSONArray driversJson = null;
		JSONArray paramsJson = null;

		try {
			JSONObject data = new JSONObject(body);
			driversJson = data.has(BODY_ATTR_DRIVERS) ? data.getJSONArray(BODY_ATTR_DRIVERS) : null;
			paramsJson = data.has(BODY_ATTR_PARAMETERS) ? data.getJSONArray(BODY_ATTR_PARAMETERS) : null;
		} catch (JSONException e) {
			String msg = String.format("Body data is invalid: %s", body);
			logger.error(msg, e);
			return Response.serverError().build();
		}

		Response ret = null;
		Locale locale = request.getLocale();
		UserProfile userProfile = UserProfileManager.getProfile();
		Map<String, String> params = manageDataSetParameters(paramsJson);
		Map<String, Object> drivers = manageDataSetDrivers(driversJson);

		try {
			Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler();

			JobDetail exportJob = ExportJobBuilder.fromDataSetIdAndUserProfile(dataSetId, userProfile).withTypeOfXls().withDrivers(drivers)
					.withParameters(params).withLocale(locale).build();

			scheduler.addJob(exportJob, true);
			scheduler.triggerJob(exportJob.getName(), exportJob.getGroup());

			ret = Response.ok().entity(exportJob.getName()).build();

			scheduleCleanUp();

		} catch (SchedulerException e) {
			String msg = String.format("Error during scheduling of export job for dataset %d", dataSetId);
			logger.error(msg, e);
			ret = Response.serverError().build();
		}

		logger.debug("OUT");

		return ret;
	}

	@GET
	@Path("/dataset/{id}")
	public Response get(@PathParam("id") UUID id) throws IOException {

		logger.debug("IN");

		Response ret = Response.status(Status.NOT_FOUND).build();

		ExportPathBuilder exportPathBuilder = ExportPathBuilder.getInstance();

		UserProfile userProfile = UserProfileManager.getProfile();
		String resoursePath = SpagoBIUtilities.getResourcePath();
		java.nio.file.Path dataFile = exportPathBuilder.getPerJobIdDataFile(resoursePath, userProfile, id);

		if (Files.isRegularFile(dataFile)) {
			java.nio.file.Path metadataFile = exportPathBuilder.getPerJobIdMetadataFile(resoursePath, userProfile, id);

			ExportMetadata metadata = ExportMetadata.readFromJsonFile(metadataFile);

			// Create a placeholder to indicate the file is downloaded
			try {
				Files.createFile(exportPathBuilder.getPerJobIdDownloadedPlaceholderFile(resoursePath, userProfile, id));
			} catch (Exception e) {
				// Yes, it's mute!
			}

			ret = Response.ok(dataFile.toFile()).header("Content-Disposition", "attachment" + "; filename=\"" + metadata.getFileName() + "\";")
					.type(metadata.getMimeType()).build();
		}

		logger.debug("OUT");

		return ret;
	}

	@POST
	@Path("/cockpitData")
	@Produces(MediaType.APPLICATION_JSON)
	public Response exportCockpitDocumentWidgetData(DocumentExportConf documentExportConf) {

		new CSVCockpitDataExporter(documentExportConf.getDocumentId(), documentExportConf.getDocumentLabel(), documentExportConf.getParameters(), null,
				UserProfileManager.getProfile(), SpagoBIUtilities.getResourcePath()).export();

		// JobDetail exportJob = new CockpitDataExportJobBuilder().setDocumentExportConf(documentExportConf).setLocale(request.getLocale())
		// .setUserProfile(UserProfileManager.getProfile()).build();
		//
		// try {
		// Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler();
		// scheduler.addJob(exportJob, true);
		// scheduler.triggerJob(exportJob.getName(), exportJob.getGroup());
		// } catch (SchedulerException e) {
		// // TODO Auto-generated catch block
		// e.printStackTrace();
		// }

		return Response.ok().entity(documentExportConf).build();

	}

	/**
	 * Manage drivers selected at client side.
	 *
	 * @param driversJson
	 *            JSON data of drivers
	 */
	private Map<String, Object> manageDataSetDrivers(JSONArray driversJson) {

		Map<String, Object> ret = new HashMap<String, Object>();

		try {
			if (driversJson != null) {
				int length = driversJson.length();
				for (int i = 0; i < length; i++) {
					JSONObject jsonObject = driversJson.getJSONObject(i);
					HashMap<String, Object> hashMapFromJSONObject = JSONObjectDeserializator.getHashMapFromJSONObject(jsonObject);
					ret.putAll(hashMapFromJSONObject);
				}
			}
		} catch (Exception e) {
			logger.debug("Cannot read dataset drivers");
			throw new IllegalStateException("Cannot read drivers");
		}

		return ret;
	}

	/**
	 * Manage parameters selected at client side.
	 *
	 * @param paramsJson
	 *            JSON data of drivers
	 */
	private Map<String, String> manageDataSetParameters(JSONArray paramsJsonArray) {

		Map<String, String> ret = new HashMap<>();

		if (paramsJsonArray != null) {
			try {
				JSONObject pars = new JSONObject();
				pars.put(DataSetConstants.PARS, paramsJsonArray);
				ManageDataSetsForREST mdsr = new ManageDataSetsForREST();
				ret = mdsr.getDataSetParametersAsMap(pars);

			} catch (Exception e) {
				logger.debug("Cannot read dataset parameters");
				throw new IllegalStateException("Cannot read dataset parameters");
			}
		}

		return ret;
	}

	/**
	 * Schedula a job to clean old export.
	 *
	 * @throws SchedulerException
	 *             In case of error during scheduling
	 */
	private void scheduleCleanUp() throws SchedulerException {

		UserProfile userProfile = UserProfileManager.getProfile();
		String resoursePath = SpagoBIUtilities.getResourcePath();

		String jobName = String.format("delete-old-export-for-%s", userProfile.getUserId());
		String jobGroup = "delete-old-export";
		String jobDescription = String.format("Delete old exports for user %s", userProfile.getUserId());

		JobDataMap jobDataMap = new JobDataMap();

		jobDataMap.put(ExportDeleteOldJob.MAP_KEY_RESOURCE_PATH, resoursePath);
		jobDataMap.put(ExportDeleteOldJob.MAP_KEY_USER_PROFILE, userProfile);

		JobDetail job = new JobDetail(jobName, jobGroup, ExportDeleteOldJob.class);

		job.setDescription(jobDescription);
		job.setJobDataMap(jobDataMap);

		Scheduler scheduler = StdSchedulerFactory.getDefaultScheduler();

		scheduler.addJob(job, true);
		scheduler.triggerJob(job.getName(), job.getGroup());

	}
}